我看到题的第一天，写了一个比较基础的版本，一开始想着先怎么优化put。

fopen一个文件，fwrite写key和value，然后fflush or fsync 落盘。

很明显每次fflush是很慢的，尝试不调用fflush马上就wrong answer了，数据出得还是比较好的。

put的瓶颈我个人觉得就是在如何优化fflush了。

题目里说明了模拟断电的方式是通过kill -9，这让我很天然的想到了mmap，由于我的工作主要和redis以及hbase相关，之前工作中经常接触mmap，知道在程序异常退出的时候，哪怕mmap的内存数据并没有落盘，kernel也会在你的进程被kill之后，回写到磁盘。这里就已经是内核态的操作了，只要服务器不真正的断电，数据的安全性是有保证的。

于是我很快就用mmap的版本替换掉了fwrite的版本，TPS在small的数据集里马上能到120w++。

再后来发现key只要你hash函数设计好，其实是不用存的，直接通过c++内置的hash把key从string类型变成了size_t。那put函数只需要一个顺序写value的文件，外加一个或者多个索引相关的index文件就行了。

index用以确定value在具体文件中的position和len。

data文件存所有的value。

这个时候就考虑怎么构造索引结构来加速get了。

一般索引主要的方式就是 平衡树 或者 hash。

平衡树索引的优势在于空间占用相对小，时间复杂度是Log n。

hash只要你开得足够大，查询就是O(1)的。

这次最终的评价标准是时间（QPS+TPS），空间只要不超就行了。因此没理由选择平衡树，我写了一个比较裸的hash就已经有非常不错的QPS了。

这个时候我的初试版本就差不多定型了。但是发现init time和memory都没有真正的用完。就想有没有可能利用init time和memory本身构造一个缓存，加速get查询。最后我用类似滚动数组的方式缓存了16段mmap的区域，每块区域16M，总共256M，外加hash表大约占用了40M，刚好用完了300M内存。这个时候已经是排第一了。

我后来想字符集既然是base64的，我应该可以用base64 encode和base64 decode去优化25%的磁盘存储，这样内存中能有更多的空间作为缓存区域，查询效率也就相对高了。然后发现这个encode和decode本身消耗了太多太多的时间。如果这是一个8核的服务器，另外7个核来做encode和decode，那这点是成立的。

另外一个小优化，在做mmap内存映射的时候，其实kernel并没有把磁盘里的数据load到内存，导致你第一次访问的时候会有个页缺失，这个时候kernel才会访问磁盘把数据拿出来，因此你的查询就有一部分时间消耗在这个页缺失上，所以在init函数里，初始化mmap的时候需要加MAP_POPULATE这个参数。这样系统会预加载这些数据到内存中，而这些时间消耗是计算在inittime里的。